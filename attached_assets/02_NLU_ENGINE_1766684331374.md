# NLU Engine - Implementation Guide

## Overview
The NLU (Natural Language Understanding) engine processes user input to extract intent, entities, sentiment, and context for conversational AI applications.

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    NLU Engine                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Input: "I want to transfer $500 to my savings account"│
│                          │                              │
│                          ▼                              │
│         ┌────────────────────────────────┐             │
│         │   Text Preprocessing           │             │
│         │   • Tokenization               │             │
│         │   • Normalization              │             │
│         │   • Language Detection         │             │
│         └────────────┬───────────────────┘             │
│                      │                                  │
│         ┌────────────▼───────────────────┐             │
│         │   Intent Classification        │             │
│         │   Confidence: 0.95             │             │
│         │   Intent: transfer_money       │             │
│         └────────────┬───────────────────┘             │
│                      │                                  │
│         ┌────────────▼───────────────────┐             │
│         │   Entity Extraction            │             │
│         │   • amount: $500               │             │
│         │   • to_account: savings        │             │
│         └────────────┬───────────────────┘             │
│                      │                                  │
│         ┌────────────▼───────────────────┐             │
│         │   Sentiment Analysis           │             │
│         │   Score: Neutral (0.1)         │             │
│         └────────────┬───────────────────┘             │
│                      │                                  │
│         ┌────────────▼───────────────────┐             │
│         │   Context Manager              │             │
│         │   • Previous: check_balance    │             │
│         │   • Current: transfer_money    │             │
│         └────────────┬───────────────────┘             │
│                      │                                  │
│                      ▼                                  │
│  Output: {                                             │
│    intent: "transfer_money",                           │
│    confidence: 0.95,                                   │
│    entities: {amount: 500, to_account: "savings"},     │
│    sentiment: "neutral",                               │
│    context: {...}                                      │
│  }                                                     │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

## Core Components

### 1. NLU Service Interface

```python
# services/nlu_service.py
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod
import asyncio

@dataclass
class NLUResult:
    """Result from NLU processing"""
    text: str
    intent: str
    confidence: float
    entities: Dict[str, Any]
    sentiment: str
    sentiment_score: float
    language: str
    context: Dict[str, Any]
    alternatives: List[Dict[str, Any]] = None

@dataclass
class Intent:
    """Intent classification result"""
    name: str
    confidence: float
    parameters: Dict[str, Any] = None

@dataclass
class Entity:
    """Extracted entity"""
    name: str
    value: Any
    type: str
    confidence: float
    start: int
    end: int
    metadata: Dict[str, Any] = None

class BaseNLUService(ABC):
    """Abstract base class for NLU services"""
    
    @abstractmethod
    async def process(self, text: str, context: Optional[Dict[str, Any]] = None) -> NLUResult:
        """Process text and return NLU results"""
        pass
    
    @abstractmethod
    async def train_model(
        self,
        agent_id: str,
        intents: List[Dict[str, Any]],
        entities: List[Dict[str, Any]]
    ) -> str:
        """Train a new NLU model"""
        pass
    
    @abstractmethod
    async def update_model(
        self,
        model_id: str,
        training_data: List[Dict[str, Any]]
    ) -> bool:
        """Update existing model with new training data"""
        pass

class CompositeNLUService(BaseNLUService):
    """
    Composite NLU service that combines multiple NLU providers
    Can use Rasa, Dialogflow, and LLM-based intent recognition
    """
    
    def __init__(
        self,
        intent_classifier,
        entity_extractor,
        sentiment_analyzer,
        context_manager,
        llm_fallback=None
    ):
        self.intent_classifier = intent_classifier
        self.entity_extractor = entity_extractor
        self.sentiment_analyzer = sentiment_analyzer
        self.context_manager = context_manager
        self.llm_fallback = llm_fallback
    
    async def process(
        self,
        text: str,
        context: Optional[Dict[str, Any]] = None,
        agent_id: Optional[str] = None
    ) -> NLUResult:
        """
        Process text through NLU pipeline
        
        Args:
            text: Input text to process
            context: Optional conversation context
            agent_id: Agent ID for model selection
        
        Returns:
            NLUResult with intent, entities, sentiment, etc.
        """
        # Preprocess text
        cleaned_text = self._preprocess(text)
        
        # Detect language
        language = await self._detect_language(cleaned_text)
        
        # Run NLU components in parallel
        intent_task = self.intent_classifier.classify(cleaned_text, agent_id)
        entities_task = self.entity_extractor.extract(cleaned_text, agent_id)
        sentiment_task = self.sentiment_analyzer.analyze(cleaned_text)
        
        intent_result, entities, sentiment = await asyncio.gather(
            intent_task,
            entities_task,
            sentiment_task
        )
        
        # Check confidence threshold
        if intent_result.confidence < 0.7 and self.llm_fallback:
            # Use LLM as fallback
            intent_result = await self._llm_intent_classification(
                cleaned_text,
                context,
                agent_id
            )
        
        # Update context
        updated_context = await self.context_manager.update(
            text=cleaned_text,
            intent=intent_result.name,
            entities=entities,
            previous_context=context
        )
        
        return NLUResult(
            text=cleaned_text,
            intent=intent_result.name,
            confidence=intent_result.confidence,
            entities={e.name: e.value for e in entities},
            sentiment=sentiment.label,
            sentiment_score=sentiment.score,
            language=language,
            context=updated_context,
            alternatives=intent_result.parameters.get("alternatives", [])
        )
    
    def _preprocess(self, text: str) -> str:
        """Preprocess text"""
        # Remove extra whitespace
        text = ' '.join(text.split())
        # Convert to lowercase for processing (preserve original for entity extraction)
        return text.strip()
    
    async def _detect_language(self, text: str) -> str:
        """Detect language of input text"""
        # Use langdetect or similar
        from langdetect import detect
        try:
            return detect(text)
        except:
            return "en"  # Default to English
    
    async def _llm_intent_classification(
        self,
        text: str,
        context: Optional[Dict[str, Any]],
        agent_id: str
    ) -> Intent:
        """Use LLM for intent classification when confidence is low"""
        # Get available intents for this agent
        available_intents = await self._get_agent_intents(agent_id)
        
        prompt = f"""
        Given the user input: "{text}"
        
        Available intents:
        {chr(10).join([f"- {intent['name']}: {intent['description']}" for intent in available_intents])}
        
        Classify the intent and extract parameters.
        Return JSON: {{"intent": "...", "confidence": 0.0-1.0, "parameters": {{}}}}
        """
        
        # Call LLM (OpenAI, Claude, etc.)
        result = await self.llm_fallback.complete(prompt)
        
        import json
        parsed = json.loads(result)
        
        return Intent(
            name=parsed["intent"],
            confidence=parsed["confidence"],
            parameters=parsed.get("parameters", {})
        )
    
    async def train_model(
        self,
        agent_id: str,
        intents: List[Dict[str, Any]],
        entities: List[Dict[str, Any]]
    ) -> str:
        """Train new NLU model"""
        # Train intent classifier
        intent_model_id = await self.intent_classifier.train(
            agent_id=agent_id,
            intents=intents
        )
        
        # Train entity extractor
        entity_model_id = await self.entity_extractor.train(
            agent_id=agent_id,
            entities=entities
        )
        
        # Save model references
        model_id = f"nlu_{agent_id}_{intent_model_id}"
        await self._save_model_reference(
            model_id=model_id,
            intent_model_id=intent_model_id,
            entity_model_id=entity_model_id
        )
        
        return model_id
    
    async def update_model(
        self,
        model_id: str,
        training_data: List[Dict[str, Any]]
    ) -> bool:
        """Update model with new training data"""
        # Implement incremental learning or retrain
        pass

# Intent Classification
class IntentClassifier:
    """Intent classification using transformer models or rule-based"""
    
    def __init__(self, model_provider="transformers"):
        self.model_provider = model_provider
        self.models = {}
    
    async def classify(
        self,
        text: str,
        agent_id: str
    ) -> Intent:
        """Classify intent from text"""
        # Load model for agent
        model = await self._get_model(agent_id)
        
        if self.model_provider == "transformers":
            return await self._classify_with_transformers(text, model)
        elif self.model_provider == "rasa":
            return await self._classify_with_rasa(text, model)
        else:
            return await self._classify_with_rules(text, model)
    
    async def _classify_with_transformers(
        self,
        text: str,
        model
    ) -> Intent:
        """Use BERT/RoBERTa for intent classification"""
        from transformers import pipeline
        
        classifier = pipeline(
            "text-classification",
            model=model,
            return_all_scores=True
        )
        
        results = classifier(text)[0]
        top_result = max(results, key=lambda x: x['score'])
        
        return Intent(
            name=top_result['label'],
            confidence=top_result['score'],
            parameters={
                'alternatives': [
                    {'intent': r['label'], 'confidence': r['score']}
                    for r in sorted(results, key=lambda x: x['score'], reverse=True)[1:4]
                ]
            }
        )
    
    async def train(
        self,
        agent_id: str,
        intents: List[Dict[str, Any]]
    ) -> str:
        """Train intent classification model"""
        # Prepare training data
        training_data = []
        for intent in intents:
            for example in intent.get('examples', []):
                training_data.append({
                    'text': example,
                    'label': intent['name']
                })
        
        # Train model (simplified)
        from sklearn.model_selection import train_test_split
        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer
        
        # Split data
        train_data, val_data = train_test_split(training_data, test_size=0.2)
        
        # Train
        model_id = f"intent_model_{agent_id}"
        # ... training code ...
        
        return model_id

# Entity Extraction
class EntityExtractor:
    """Extract entities using NER models"""
    
    def __init__(self):
        self.models = {}
    
    async def extract(
        self,
        text: str,
        agent_id: str
    ) -> List[Entity]:
        """Extract entities from text"""
        model = await self._get_model(agent_id)
        
        # Use spaCy or transformers for NER
        import spacy
        nlp = spacy.load("en_core_web_sm")
        doc = nlp(text)
        
        entities = []
        for ent in doc.ents:
            entities.append(Entity(
                name=ent.label_,
                value=ent.text,
                type=ent.label_,
                confidence=1.0,  # spaCy doesn't provide confidence
                start=ent.start_char,
                end=ent.end_char
            ))
        
        # Also extract custom entities (amounts, account types, etc.)
        entities.extend(await self._extract_custom_entities(text, agent_id))
        
        return entities
    
    async def _extract_custom_entities(
        self,
        text: str,
        agent_id: str
    ) -> List[Entity]:
        """Extract custom entities specific to agent"""
        entities = []
        
        # Extract amounts
        import re
        amount_pattern = r'\$?([\d,]+(?:\.\d{2})?)'
        for match in re.finditer(amount_pattern, text):
            amount = float(match.group(1).replace(',', ''))
            entities.append(Entity(
                name='amount',
                value=amount,
                type='currency',
                confidence=1.0,
                start=match.start(),
                end=match.end()
            ))
        
        # Extract account types
        account_types = ['checking', 'savings', 'credit']
        for acc_type in account_types:
            if acc_type in text.lower():
                idx = text.lower().index(acc_type)
                entities.append(Entity(
                    name='account_type',
                    value=acc_type,
                    type='account',
                    confidence=1.0,
                    start=idx,
                    end=idx + len(acc_type)
                ))
        
        return entities

# Sentiment Analysis
class SentimentAnalyzer:
    """Analyze sentiment of user input"""
    
    async def analyze(self, text: str):
        """Analyze sentiment"""
        from transformers import pipeline
        
        sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
        
        result = sentiment_pipeline(text)[0]
        
        return SentimentResult(
            label=result['label'],
            score=result['score']
        )

@dataclass
class SentimentResult:
    label: str
    score: float

# Context Management
class ContextManager:
    """Manage conversation context"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def update(
        self,
        text: str,
        intent: str,
        entities: List[Entity],
        previous_context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Update conversation context"""
        context = previous_context or {}
        
        # Add current turn
        context['last_intent'] = intent
        context['last_entities'] = {e.name: e.value for e in entities}
        context['turn_count'] = context.get('turn_count', 0) + 1
        
        # Track intent history
        if 'intent_history' not in context:
            context['intent_history'] = []
        context['intent_history'].append(intent)
        
        # Store in Redis if session_id provided
        if session_id:
            await self.redis.setex(
                f"context:{session_id}",
                3600,  # 1 hour TTL
                json.dumps(context)
            )
        
        return context
    
    async def get(self, session_id: str) -> Dict[str, Any]:
        """Get context for session"""
        data = await self.redis.get(f"context:{session_id}")
        if data:
            return json.loads(data)
        return {}
```

### 2. API Endpoints

```python
# api/routes/nlu.py
from fastapi import APIRouter, HTTPException, Depends
from typing import Optional
from services.nlu_service import CompositeNLUService, NLUResult
from pydantic import BaseModel

router = APIRouter(prefix="/api/v1/nlu", tags=["NLU"])

class NLURequest(BaseModel):
    text: str
    agent_id: str
    session_id: Optional[str] = None
    context: Optional[dict] = None

class TrainingRequest(BaseModel):
    agent_id: str
    intents: list
    entities: list

@router.post("/process", response_model=NLUResult)
async def process_text(
    request: NLURequest,
    nlu_service: CompositeNLUService = Depends()
):
    """Process text through NLU pipeline"""
    try:
        # Get context if session_id provided
        context = request.context
        if request.session_id:
            context = await nlu_service.context_manager.get(request.session_id)
        
        result = await nlu_service.process(
            text=request.text,
            context=context,
            agent_id=request.agent_id
        )
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/train")
async def train_model(
    request: TrainingRequest,
    nlu_service: CompositeNLUService = Depends()
):
    """Train NLU model for an agent"""
    try:
        model_id = await nlu_service.train_model(
            agent_id=request.agent_id,
            intents=request.intents,
            entities=request.entities
        )
        return {"model_id": model_id, "status": "training_complete"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/intents/{agent_id}")
async def get_agent_intents(agent_id: str):
    """Get configured intents for an agent"""
    # Retrieve from database
    intents = await intent_repository.get_by_agent(agent_id)
    return intents

@router.post("/feedback")
async def submit_feedback(
    session_id: str,
    intent: str,
    correct_intent: str,
    text: str
):
    """Submit feedback for model improvement"""
    await feedback_repository.save({
        'session_id': session_id,
        'predicted_intent': intent,
        'correct_intent': correct_intent,
        'text': text,
        'timestamp': datetime.utcnow()
    })
    return {"status": "feedback_recorded"}
```

### 3. Frontend Components

```typescript
// components/NLU/NLUTester.tsx
import React, { useState } from 'react';
import {
  Paper,
  TextField,
  Button,
  Typography,
  Chip,
  Box,
  CircularProgress
} from '@mui/material';
import { processNLU } from '../../api/nlu';

export const NLUTester: React.FC<{ agentId: string }> = ({ agentId }) => {
  const [text, setText] = useState('');
  const [result, setResult] = useState(null);
  const [loading, setLoading] = useState(false);

  const handleTest = async () => {
    setLoading(true);
    try {
      const nluResult = await processNLU({
        text,
        agent_id: agentId
      });
      setResult(nluResult);
    } catch (error) {
      console.error('NLU processing failed:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <Paper sx={{ p: 3 }}>
      <Typography variant="h6" gutterBottom>
        Test NLU Engine
      </Typography>
      
      <TextField
        fullWidth
        multiline
        rows={3}
        value={text}
        onChange={(e) => setText(e.target.value)}
        placeholder="Enter text to test NLU..."
        sx={{ mb: 2 }}
      />
      
      <Button
        variant="contained"
        onClick={handleTest}
        disabled={!text || loading}
      >
        {loading ? <CircularProgress size={24} /> : 'Analyze'}
      </Button>
      
      {result && (
        <Box sx={{ mt: 3 }}>
          <Typography variant="subtitle1">Results:</Typography>
          
          <Box sx={{ mt: 2 }}>
            <Typography variant="body2">Intent:</Typography>
            <Chip 
              label={`${result.intent} (${(result.confidence * 100).toFixed(1)}%)`}
              color="primary"
              sx={{ mt: 1 }}
            />
          </Box>
          
          <Box sx={{ mt: 2 }}>
            <Typography variant="body2">Entities:</Typography>
            {Object.entries(result.entities).map(([key, value]) => (
              <Chip
                key={key}
                label={`${key}: ${value}`}
                sx={{ mt: 1, mr: 1 }}
              />
            ))}
          </Box>
          
          <Box sx={{ mt: 2 }}>
            <Typography variant="body2">Sentiment:</Typography>
            <Chip
              label={`${result.sentiment} (${(result.sentiment_score * 100).toFixed(1)}%)`}
              sx={{ mt: 1 }}
            />
          </Box>
        </Box>
      )}
    </Paper>
  );
};
```

## Training Data Format

```json
{
  "intents": [
    {
      "name": "transfer_money",
      "examples": [
        "Transfer $500 to my savings account",
        "Move 100 dollars from checking to savings",
        "I want to send money to my other account"
      ],
      "slots": ["amount", "from_account", "to_account"]
    }
  ],
  "entities": [
    {
      "name": "amount",
      "type": "currency",
      "examples": [
        {"text": "Transfer $500", "value": 500, "start": 9, "end": 13}
      ]
    }
  ]
}
```

This NLU engine provides robust intent classification, entity extraction, and context management. Continue with Orchestrator next?
